{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5effa009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.3.6\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54e92569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random \n",
    "from string import ascii_letters\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn \n",
    "import  torch.nn.functional as F \n",
    "from unidecode import unidecode\n",
    "\n",
    "_ = torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1978cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  data.zip\tRNN-scratch.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a14cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Spanish': tensor([0]),\n",
       " 'Korean': tensor([1]),\n",
       " 'Chinese': tensor([2]),\n",
       " 'French': tensor([3]),\n",
       " 'Portuguese': tensor([4]),\n",
       " 'Russian': tensor([5]),\n",
       " 'Czech': tensor([6]),\n",
       " 'Vietnamese': tensor([7]),\n",
       " 'Dutch': tensor([8]),\n",
       " 'Japanese': tensor([9]),\n",
       " 'Polish': tensor([10]),\n",
       " 'Arabic': tensor([11]),\n",
       " 'Scottish': tensor([12]),\n",
       " 'German': tensor([13]),\n",
       " 'English': tensor([14]),\n",
       " 'Italian': tensor([15]),\n",
       " 'Irish': tensor([16]),\n",
       " 'Greek': tensor([17])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = './data/names'\n",
    "lang2label = {\n",
    "    file_name.split(\".\")[0]: torch.tensor([i], dtype=torch.long)\n",
    "    for i, file_name in enumerate(os.listdir(data_dir))\n",
    "}\n",
    "lang2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecbc20c",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faca9082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Slusarski'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unidecode(\"Ślusàrski\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c16f0c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aNgrejiimaa'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unidecode(\"अंग्रेजीमा\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "894f7312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx = {letter: i for i, letter in enumerate(ascii_letters + \" .,:;-'\")}\n",
    "num_letters = len(char2idx); num_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2acf254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name2tensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, num_letters)\n",
    "    for i, char in enumerate(name):\n",
    "        tensor[i][0][char2idx[char]] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49339ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "name2tensor(\"abc\").numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67add0a",
   "metadata": {},
   "source": [
    "## Dataset Creation::\n",
    "from `german.txt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f1fc72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_names = []\n",
    "target_langs = []\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    with open(os.path.join(data_dir, file)) as f:\n",
    "        lang = file.split(\".\")[0]\n",
    "        names = [unidecode(line.rstrip()) for line in f]\n",
    "        for name in names:\n",
    "            try:\n",
    "                tensor_names.append(name2tensor(name))\n",
    "                target_langs.append(lang2label[lang])\n",
    "            except KeyError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34639d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_names[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "662fe777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = [\n",
    "#     [tensor_names[i] , target_langs[i]]  for i in range(len(tensor_names))\n",
    "# ]\n",
    "X_train , X_test , y_train , y_test = train_test_split(\n",
    "    tensor_names , target_langs,test_size=0.2,random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3a90384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_idx, test_idx = train_test_split(\n",
    "#     range(len(target_langs)), \n",
    "#     test_size=0.1, \n",
    "#     shuffle=True, \n",
    "#     stratify=target_langs\n",
    "# )\n",
    "# (train_idx , test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9ba21a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_idx, test_idx = train_test_split(\n",
    "#     range(len(target_langs)), \n",
    "#     test_size=0.1, \n",
    "#     shuffle=True, \n",
    "#     stratify=target_langs\n",
    "# )\n",
    "\n",
    "# train_dataset = [\n",
    "#     (tensor_names[i], target_langs[i])\n",
    "#     for i in train_idx\n",
    "# ]\n",
    "\n",
    "# test_dataset = [\n",
    "#     (tensor_names[i], target_langs[i])\n",
    "#     for i in test_idx\n",
    "# ]\n",
    "train_dataset = [(X_train , y_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "293f4761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7fdfa1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 16056\n",
      "Test: 4014\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {len(X_train)}\")\n",
    "print(f\"Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bdf9b8",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "a simple RNN model from scratch using Pytorch \n",
    "\n",
    "nn.init to initialize at the hidden layers \n",
    "kaiming_uniform to initialze hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "3cd053cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size:int, hidden_size:int , output_size:int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.inp2hidden = nn.Linear(input_size , hidden_size , bias=False)\n",
    "        self.hid2out = nn.Linear(hidden_size , output_size)\n",
    "        \n",
    "    \n",
    "    def forward(self , x, hidden_state):\n",
    "        \"\"\"\n",
    "        Returns computed output and tanh(i2h + h2h)\n",
    "        Inputs\n",
    "        ------\n",
    "        x: Input vector\n",
    "        hidden_state: Previous hidden state\n",
    "        Outputs\n",
    "        -------\n",
    "        out: Linear output (without activation because of how pytorch works)\n",
    "        hidden_state: New hidden state matrix\n",
    "        \"\"\"\n",
    "        x = self.inp2hidden(x)\n",
    "        hidden_state = torch.tanh(x+hidden_state)\n",
    "        out  = self.hid2out(hidden_state)\n",
    "        return out , hidden_state\n",
    "    \n",
    "    def init_zero_hidden(self , batch_size = 1):\n",
    "        \"\"\"\n",
    "        Helper function.\n",
    "        Returns a hidden state with specified batch size. Defaults to 1\n",
    "        \n",
    "        \"\"\"\n",
    "        return torch.zeros(batch_size, self.hidden_size, requires_grad=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "6bf6072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SimpleRNN(nn.Module):\n",
    "#     def __init__(self , input_size , hidden_size , output_size):\n",
    "#         super().__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.inp2hidden = nn.Linear(input_size+hidden_size , hidden_size)\n",
    "#         self.inp2output = nn.Linear(input_size+hidden_size  ,output_size)\n",
    "        \n",
    "#     def forward(self , x , hidden_state):\n",
    "#         print(\"inside forward\")\n",
    "#         print(x.shape)\n",
    "#         print(hidden_state.shape)\n",
    "#         combined = torch.cat((x,hidden_state),1 )\n",
    "        \n",
    "#         hidden = torch.sigmoid(self.inp2hidden(combined))\n",
    "#         output = self.inp2output(combined)\n",
    "        \n",
    "#         return output,hidden\n",
    "    \n",
    "#     def init_hidden(self):\n",
    "#         return nn.init.kaiming_uniform_(torch.empty(9,1 , 59))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "bded96d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "learning_rate = 0.001\n",
    "num_langs = len(lang2label)\n",
    "print(num_letters)\n",
    "model = SimpleRNN(num_letters, hidden_size, num_langs)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "bad75715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 59])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_names[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "14d9763c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.init_zero_hidden().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "4d0d3329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "a3ac264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 1, 59])\n",
      "torch.Size([1, 256])\n",
      "torch.Size([7, 1, 59])\n",
      "torch.Size([9, 1, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (7) must match the size of tensor b (9) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[312], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m (char\u001b[38;5;241m.\u001b[39mshape)    \n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(hidden_state\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 11\u001b[0m     output, hidden_state \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, label)\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Desktop/NLP/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[306], line 26\u001b[0m, in \u001b[0;36mSimpleRNN.forward\u001b[0;34m(self, x, hidden_state)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03mReturns computed output and tanh(i2h + h2h)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03mInputs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03mhidden_state: New hidden state matrix\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minp2hidden(x)\n\u001b[0;32m---> 26\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[43mx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mhidden_state\u001b[49m)\n\u001b[1;32m     27\u001b[0m out  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhid2out(hidden_state)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out , hidden_state\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (7) must match the size of tensor b (9) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "print_interval = 3000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "#     random.shuffle(X_train)\n",
    "    for i , (name, label) in enumerate(train_dataset):\n",
    "        hidden_state = model.init_zero_hidden()\n",
    "        for char in name:\n",
    "            print (char.shape)    \n",
    "            print(hidden_state.shape)\n",
    "            output, hidden_state = model(char, hidden_state)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % print_interval == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "                f\"Step [{i + 1}/{len(train_dataset)}], \"\n",
    "                f\"Loss: {loss.item():.4f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e985339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d5edaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
