{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "84163f3ca19c0b7c9fda47121b3bc4cadfaf1fcc"
   },
   "source": [
    "# Gensim Word2Vec Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6721f3a77cbc903d996b88c42f22c3e5e920e909"
   },
   "source": [
    "\n",
    "<img src=\"https://images.freeimages.com/images/large-previews/2b9/letters-1-yes-1188348.jpg\" alt=\"drawing\" width=\"350\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9e858d8c6e56ee78629e1cc21ce8fee6dc3cec0b"
   },
   "source": [
    "# Motivation\n",
    "\n",
    "As I started working at [Supportiv](http://www.supportiv.com), the support network for instant peer support, a few months ago, I began looking into Language Models and Word2Vec particularly. A python native, I naturally decided to focus on Gensim's implementation of Word2Vec, and went on to look for tutorials on the web. As all good data scientists, I directly applied and reproduced the code samples from multiple website. Confused and often disappointed by the results I got, I went deeper and deeper, from stackoverflow threads, to Gensim's Google Groups, onto the documentation of the library, to try and understand what went wrong in my approach. After weeks of hard labor, I finally managed to get decent results, but I was frustrated by these online tutorials which were, for the most part, misleading.\n",
    "\n",
    "What troubled me the most in these online tutorials was their mismanagement of the model training: the code worked, and I got results which appeared to be decent at first, but the more I looked into them, the more disturbing they were. I wasted a lot of time figuring out what was wrong.\n",
    "\n",
    "Another issue I had with these tutorials was the data preparation step: too often, the authors chose to load an existing preprocessed dataset, use a toy example or skip this part. However, I always thought that one of the most important parts of the creation of a Word2Vec model was then missing. During my experimentations, I noticed that lemmatizing the sentences or looking for phrases/bigrams in them had a big impact over the results and performance of my models. Though the influence of the preprocessing varies with each dataset and application, I thought I would include the data preparation steps in this tutorial and use the great spaCy library along with it.\n",
    "\n",
    "I am not the only one annoyed by some of these [issues](https://groups.google.com/d/msg/gensim/jom4JFt7EV8/y5fjhupbAgAJ), so I decided to write my own tutorial.\n",
    "\n",
    "I do not pledge that it is perfect, nor the best way to implement Word2Vec, simply that it is better than a good chunk of what is out there 😉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aa10f4823faa729e96f15bb865e9f20769ea7a4b"
   },
   "source": [
    "# Plan\n",
    "\n",
    "1. [Briefing about Word2Vec](#Briefing-about-Word2Vec:)\n",
    "    * [Purpose of the tutorial](#Purpose-of-the-tutorial:)\n",
    "    * [Brief explanation](#Brief-explanation:)\n",
    "\n",
    "2. [Getting Started](#Getting-Started)\n",
    "    * [Setting up the environment](#Setting-up-the-environment:)\n",
    "    * [The data](#The-data:)\n",
    "3. [Preprocessing](#Preprocessing)\n",
    "    * [Cleaning](#Cleaning)\n",
    "    * [Bigrams](#Bigrams)\n",
    "    * [Most frequent words](#Most-Frequent-Words)\n",
    "    \n",
    "4. [Training the Model](#Training-the-model)\n",
    "    * [Gensim Word2Vec Implementation](#Gensim-Word2Vec-Implementation:)\n",
    "    * [Why I seperate the training of the model in 3 steps](#Why-I-seperate-the-training-of-the-model-in-3-steps:)\n",
    "    * [Training the model](#Training-the-model)\n",
    "        * [The parameters](#The-parameters)\n",
    "        * [Building the vocabulary table](#Building-the-Vocabulary-Table)\n",
    "        * [Training of the model](#Training-of-the-model)\n",
    "        * [Saving the model](#Saving-the-model:)\n",
    "5. [Exploring the Model](#Exploring-the-model)\n",
    "    * [Most similar to](#Most-similar-to:)\n",
    "    * [Similarities](#Similarities:)\n",
    "    * [Odd-one-out](#Odd-One-Out:)\n",
    "    * [Analogy difference](#Analogy-difference:)\n",
    "    * [t-SNE visualizations](#t-SNE-visualizations:)\n",
    "        * [10 Most similar words vs. 8 Random words](#10-Most-similar-words-vs.-8-Random-words:)\n",
    "        * [10 Most similar words vs. 10 Most dissimilar](#10-Most-similar-words-vs.-10-Most-dissimilar:)\n",
    "        * [10 Most similar words vs. 11th to 20th Most similar words](#10-Most-similar-words-vs.-11th-to-20th-Most-similar-words:)\n",
    "6. [Final Thoughts](#Final-Thoughts)\n",
    "7. [Material for more in depths understanding](#Material-for-more-in-depths-understanding:)\n",
    "8. [Acknowledgements](#Acknowledgements)\n",
    "9. [References](#References:)\n",
    "10. [End](#End)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7d96105f0c90bf052b2afdb684bf31549e1e6c81"
   },
   "source": [
    "# Briefing about Word2Vec:\n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png\" alt=\"drawing\" width=\"550\"/>\n",
    "\n",
    "[[1]](#References:)\n",
    "\n",
    "\n",
    "## Purpose of the tutorial:\n",
    "As I said before, this tutorial focuses on the right use of the Word2Vec package from the Gensim libray; therefore, I am not going to explain the concepts and ideas behind Word2Vec here. I am simply going to give a very brief explanation, and provide you with links to good, in depth tutorials.\n",
    "\n",
    "## Brief explanation:\n",
    "\n",
    "Word2Vec was introduced in two [papers](#Material-for-more-in-depths-understanding:) between September and October 2013, by a team of researchers at Google. Along with the papers, the researchers published their implementation in C. The Python implementation was done soon after the 1st paper, by [Gensim](https://radimrehurek.com/gensim/index.html). \n",
    "\n",
    "The underlying assumption of Word2Vec is that two words sharing similar contexts also share a similar meaning and consequently a similar vector representation from the model. For instance: \"dog\", \"puppy\" and \"pup\" are often used in similar situations, with similar surrounding words like \"good\", \"fluffy\" or \"cute\", and according to Word2Vec they will therefore share a similar vector representation.<br>\n",
    "\n",
    "From this assumption, Word2Vec can be used to find out the relations between words in a dataset, compute the similarity between them, or use the vector representation of those words as input for other applications such as text classification or clustering.\n",
    "\n",
    "# Getting Started\n",
    "\n",
    "## Setting up the environment:\n",
    "\n",
    "`python==3.6.3`\n",
    "\n",
    "Libraries used:\n",
    " * `xlrd==1.1.0`: https://pypi.org/project/xlrd/\n",
    " * `spaCy==2.0.12`: https://spacy.io/usage/\n",
    " * `gensim==3.4.0`: https://radimrehurek.com/gensim/install.html\n",
    " * `scikit-learn==0.19.1`: http://scikit-learn.org/stable/install.html\n",
    " * `seaborn==0.8`: https://seaborn.pydata.org/installing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 KB\u001b[0m \u001b[31m747.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 KB\u001b[0m \u001b[31m952.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from spacy) (23.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: jinja2 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (913 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.3/913.3 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.2/492.2 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from spacy) (59.6.0)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from spacy) (4.65.0)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.6.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, pydantic, murmurhash, langcodes, catalogue, blis, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.9 spacy-3.5.3 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.10 typer-0.7.0 wasabi-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "cc7b3e6ca62670ff13626705402f626778487204"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:651: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    }
   ],
   "source": [
    "import re  # For preprocessing\n",
    "import pandas as pd  # For data handling\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "import spacy  # For preprocessing\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8ec7ce784f6d9e7f71e2b5789b1e65ec4414628b"
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/44/Logo_The_Simpsons.svg\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "## The data:\n",
    "I chose to play with the script from the Simpsons, both because I love the Simpsons and because with more than 150k lines of dialogues, the dataset was substantial!\n",
    "\n",
    "This dataset contains the characters, locations, episode details, and script lines for approximately 600 Simpsons episodes, dating back to 1989. It can be found here: https://www.kaggle.com/ambarish/fun-in-text-mining-with-simpsons/data (~25MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0c36323d9aa62f74ab348cda5ee0f571aa1d4a96"
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "We keep only two columns:\n",
    "* `raw_character_text`: the character who speaks (can be useful when monitoring the preprocessing steps)\n",
    "* `spoken_words`: the raw text from the line of dialogue\n",
    "\n",
    "We do not keep `normalized_text` because we want to do our own preprocessing.\n",
    "\n",
    "You can find the resulting file here: https://www.kaggle.com/pierremegret/dialogue-lines-of-the-simpsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "6453b9c3f797e51923e030090ead659253f4e459"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158314, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./simpsons_dataset.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "c6c6bf4462fb4bc00c2abdbf65eced888219f364"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        raw_character_text                                       spoken_words\n",
       "0              Miss Hoover  No, actually, it was a little of both. Sometim...\n",
       "1             Lisa Simpson                             Where's Mr. Bergstrom?\n",
       "2              Miss Hoover  I don't know. Although I'd sure like to talk t...\n",
       "3             Lisa Simpson                         That life is worth living.\n",
       "4  Edna Krabappel-Flanders  The polls will be open from now until the end ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "caf6838463d56f79e92d87d5a3827fcd5f04fc54"
   },
   "source": [
    "The missing values comes from the part of the script where something happens, but with no dialogue. For instance \"(Springfield Elementary School: EXT. ELEMENTARY - SCHOOL PLAYGROUND - AFTERNOON)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "3a15727caeba1c8d10573456640d0b8b9f2f2e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_character_text    17814\n",
       "spoken_words          26459\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "885a555596d7484841ea54c94405d03d90572396"
   },
   "source": [
    "Removing the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "82cb38f176526679f66ee31e11cfe4f5eebdab51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_character_text    0\n",
       "spoken_words          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna().reset_index(drop=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7f07dca2a2656dcd9e0c315afa36af32a992eef7"
   },
   "source": [
    "## Cleaning:\n",
    "We are lemmatizing and removing the stopwords and non-alphabetic characters for each line of dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to download the spacy model\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "b26a0c01c5701630d3951cfc808a9d944eea6371"
   },
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
    "nlp = spacy.load(\"en_core_web_sm\" , disable=['ner' , 'parser'])\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords , \n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f686964722eede40e5961cd232aee7b6dd587bd1"
   },
   "source": [
    "Removes non-alphabetic characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "b45598934171607242ca7d50f8c5f7c91411aace"
   },
   "outputs": [],
   "source": [
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['spoken_words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2360160a7f326a32d56f2f18782d7ce2f4ac1def"
   },
   "source": [
    "Taking advantage of spaCy .pipe() attribute to speed-up the cleaning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "fa44ca458c970ca229426779e6ffcd46c2de313c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 1.76 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000)]\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "65ec76c60f9fe93bba6909f4e696f90e3e54710b"
   },
   "source": [
    "Put the results in a DataFrame to remove missing values and duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "57f1eb8382554bc592d48915a903230b5b6d6cf7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>actually little disease magazine news show nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>know sure like talk touch lesson plan teach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>life worth live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>poll open end recess case decide thought final...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>victory party slide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               clean\n",
       "0  actually little disease magazine news show nat...\n",
       "2        know sure like talk touch lesson plan teach\n",
       "3                                    life worth live\n",
       "4  poll open end recess case decide thought final...\n",
       "7                                victory party slide"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "31b4a744059df490ddb47ab6cdec008dc929ede3"
   },
   "source": [
    "## Bigrams:\n",
    "We are using Gensim Phrases package to automatically detect common phrases (bigrams) from a list of sentences.\n",
    "https://radimrehurek.com/gensim/models/phrases.html\n",
    "\n",
    "The main reason we do this is to catch words like \"mr_burns\" or \"bart_simpson\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "af6d420284a0ff7a7407d4c526754ffe850d6170"
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "788aec3c82788101db25d4ca6105ee133fecae7c"
   },
   "source": [
    "As `Phrases()` takes a list of list of words as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "f58487ff08d8812622fd7aef36139f1c850add18"
   },
   "outputs": [],
   "source": [
    "sent = [row.split() for row in df_clean['clean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bb7766b322cbc1d3381912b890585eb249ac5304"
   },
   "source": [
    "Creates the relevant phrases from the list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "8befad8c76c54bd2b831b0942a2f626f7d8a6dac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 20:17:55: collecting all words and their counts\n",
      "INFO - 20:17:55: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 20:17:55: PROGRESS: at sentence #10000, processed 63557 words and 52723 word types\n",
      "INFO - 20:17:55: PROGRESS: at sentence #20000, processed 130936 words and 99612 word types\n",
      "INFO - 20:17:55: PROGRESS: at sentence #30000, processed 192961 words and 138181 word types\n",
      "INFO - 20:17:55: PROGRESS: at sentence #40000, processed 249832 words and 172156 word types\n",
      "INFO - 20:17:55: PROGRESS: at sentence #50000, processed 311269 words and 207943 word types\n",
      "INFO - 20:17:55: PROGRESS: at sentence #60000, processed 373578 words and 242950 word types\n",
      "INFO - 20:17:55: PROGRESS: at sentence #70000, processed 436424 words and 277852 word types\n",
      "INFO - 20:17:56: PROGRESS: at sentence #80000, processed 497887 words and 310927 word types\n",
      "INFO - 20:17:56: collected 329641 token types (unigram + bigrams) from a corpus of 537095 words and 85955 sentences\n",
      "INFO - 20:17:56: merged Phrases<329641 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 20:17:56: Phrases lifecycle event {'msg': 'built Phrases<329641 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.71s', 'datetime': '2023-06-08T20:17:56.086504', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-43-generic-x86_64-with-glibc2.35', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(sent, min_count=30, progress_per=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "45bae4a953f2ad8951e4efb234e1e357857a33b3"
   },
   "source": [
    "The goal of Phraser() is to cut down memory consumption of Phrases(), by discarding model state not strictly needed for the bigram detection task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "b8ae81ba230013aefe7c584338de7376fedf6294"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 20:18:11: exporting phrases from Phrases<329641 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 20:18:12: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<128 phrases, min_count=30, threshold=10.0> from Phrases<329641 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.56s', 'datetime': '2023-06-08T20:18:12.066169', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-43-generic-x86_64-with-glibc2.35', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a58380f19d159688aeee665d1afb96289fdd4b8"
   },
   "source": [
    "Transform the corpus based on the bigrams detected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "8051b56890c147119db3df529d3cfd3cf675fdca"
   },
   "outputs": [],
   "source": [
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4f81e8bb2c09a67b00cd24db28353eca8ae188c"
   },
   "source": [
    "## Most Frequent Words:\n",
    "Mainly a sanity check of the effectiveness of the lemmatization, removal of stopwords, and addition of bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "eeb8afe1cfcb7ba65bd14d657455600acacf39ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29694"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "5b010149150b2b2eaf332d79bcde0649b8a3c2b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh', 'like', 'know', 'get', 'hey', 'think', 'come', 'right', 'look', 'want']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "500ab7b5c84dc006d7945f339c40725a82856fdf"
   },
   "source": [
    "# Training the model\n",
    "## Gensim Word2Vec Implementation:\n",
    "We use Gensim implementation of word2vec: https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "3269be205cadbad499aa87890893d92da6adc796"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c524bc49c41a6c37f9e754a38797c9501202090"
   },
   "source": [
    "## Why I seperate the training of the model in 3 steps:\n",
    "I prefer to separate the training in 3 distinctive steps for clarity and monitoring.\n",
    "1. `Word2Vec()`: \n",
    ">In this first step, I set up the parameters of the model one-by-one. <br>I do not supply the parameter `sentences`, and therefore leave the model uninitialized, purposefully.\n",
    "2. `.build_vocab()`: \n",
    ">Here it builds the vocabulary from a sequence of sentences and thus initialized the model. <br>With the loggings, I can follow the progress and even more important, the effect of `min_count` and `sample` on the word corpus. I noticed that these two parameters, and in particular `sample`, have a great influence over the performance of a model. Displaying both allows for a more accurate and an easier management of their influence.\n",
    "3. `.train()`:\n",
    ">Finally, trains the model.<br>\n",
    "The loggings here are mainly useful for monitoring, making sure that no threads are executed instantaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "03488d9b68963579c96094aca88a302c9f2753a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "89c305fcd163488441ac2ac6133678bd973b4419"
   },
   "source": [
    "## The parameters:\n",
    "\n",
    "* `min_count` <font color='purple'>=</font> <font color='green'>int</font> - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
    "\n",
    "\n",
    "* `window` <font color='purple'>=</font> <font color='green'>int</font> - The maximum distance between the current and predicted word within a sentence. E.g. `window` words on the left and `window` words on the left of our target - (2, 10)\n",
    "\n",
    "\n",
    "* `size` <font color='purple'>=</font> <font color='green'>int</font> - Dimensionality of the feature vectors. - (50, 300)\n",
    "\n",
    "\n",
    "* `sample` <font color='purple'>=</font> <font color='green'>float</font> - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial.  - (0, 1e-5)\n",
    "\n",
    "\n",
    "* `alpha` <font color='purple'>=</font> <font color='green'>float</font> - The initial learning rate - (0.01, 0.05)\n",
    "\n",
    "\n",
    "* `min_alpha` <font color='purple'>=</font> <font color='green'>float</font> - Learning rate will linearly drop to `min_alpha` as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
    "\n",
    "\n",
    "* `negative` <font color='purple'>=</font> <font color='green'>int</font> - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
    "\n",
    "\n",
    "* `workers` <font color='purple'>=</font> <font color='green'>int</font> - Use these many worker threads to train the model (=faster training with multicore machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "ad619db82c219d6cb81fad516563feb0c4d474cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 20:21:48: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.03>', 'datetime': '2023-06-08T20:21:48.388094', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-43-generic-x86_64-with-glibc2.35', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d7e9f1bd338f9e15647b5209ffd8fbb131cd7ee5"
   },
   "source": [
    "## Building the Vocabulary Table:\n",
    "Word2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "66358ad743e05e17dfbed3899af9c41056143daa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 20:23:02: collecting all words and their counts\n",
      "INFO - 20:23:02: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 20:23:02: PROGRESS: at sentence #10000, processed 61697 words, keeping 9518 word types\n",
      "INFO - 20:23:02: PROGRESS: at sentence #20000, processed 127312 words, keeping 14384 word types\n",
      "INFO - 20:23:02: PROGRESS: at sentence #30000, processed 187772 words, keeping 17442 word types\n",
      "INFO - 20:23:02: PROGRESS: at sentence #40000, processed 243265 words, keeping 20121 word types\n",
      "INFO - 20:23:02: PROGRESS: at sentence #50000, processed 303120 words, keeping 22551 word types\n",
      "INFO - 20:23:02: PROGRESS: at sentence #60000, processed 363858 words, keeping 24820 word types\n",
      "INFO - 20:23:02: PROGRESS: at sentence #70000, processed 425311 words, keeping 26987 word types\n",
      "INFO - 20:23:02: PROGRESS: at sentence #80000, processed 485433 words, keeping 28822 word types\n",
      "INFO - 20:23:02: collected 29694 word types from a corpus of 523538 raw words and 85955 sentences\n",
      "INFO - 20:23:02: Creating a fresh vocabulary\n",
      "INFO - 20:23:02: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 3325 unique words (11.20% of original 29694, drops 26369)', 'datetime': '2023-06-08T20:23:02.836010', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-43-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "INFO - 20:23:02: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 438016 word corpus (83.66% of original 523538, drops 85522)', 'datetime': '2023-06-08T20:23:02.836895', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-43-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "INFO - 20:23:02: deleting the raw counts dictionary of 29694 items\n",
      "INFO - 20:23:02: sample=6e-05 downsamples 1198 most-common words\n",
      "INFO - 20:23:02: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 199427.65110464947 word corpus (45.5%% of prior 438016)', 'datetime': '2023-06-08T20:23:02.857941', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-43-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "INFO - 20:23:02: estimated required memory for 3325 words and 100 dimensions: 4322500 bytes\n",
      "INFO - 20:23:02: resetting layer weights\n",
      "INFO - 20:23:02: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-06-08T20:23:02.904515', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-43-generic-x86_64-with-glibc2.35', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "63260d82061abb47db7f2f8b23e07ec629adf5a9"
   },
   "source": [
    "## Training of the model:\n",
    "_Parameters of the training:_\n",
    "* `total_examples` <font color='purple'>=</font> <font color='green'>int</font> - Count of sentences;\n",
    "* `epochs` <font color='purple'>=</font> <font color='green'>int</font> - Number of iterations (epochs) over the corpus - [10, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "07a2a047e701e512fd758edff186daadaeea6461"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 20:23:50: Word2Vec lifecycle event {'msg': 'training model with 7 workers on 3325 vocabulary and 100 features, using sg=0 hs=0 sample=6e-05 negative=20 window=2 shrink_windows=True', 'datetime': '2023-06-08T20:23:50.299397', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-43-generic-x86_64-with-glibc2.35', 'event': 'train'}\n",
      "INFO - 20:23:51: EPOCH 0: training on 523538 raw words (199488 effective words) took 0.7s, 270243 effective words/s\n",
      "INFO - 20:23:52: EPOCH 1 - PROGRESS: at 91.99% examples, 180713 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 20:23:52: EPOCH 1: training on 523538 raw words (199423 effective words) took 1.1s, 187434 effective words/s\n",
      "INFO - 20:23:52: EPOCH 2: training on 523538 raw words (199123 effective words) took 0.7s, 275641 effective words/s\n",
      "INFO - 20:23:53: EPOCH 3: training on 523538 raw words (199356 effective words) took 0.9s, 214100 effective words/s\n",
      "INFO - 20:23:54: EPOCH 4: training on 523538 raw words (199450 effective words) took 0.9s, 224573 effective words/s\n",
      "INFO - 20:23:55: EPOCH 5: training on 523538 raw words (199140 effective words) took 0.7s, 290656 effective words/s\n",
      "INFO - 20:23:56: EPOCH 6: training on 523538 raw words (199657 effective words) took 0.9s, 221202 effective words/s\n",
      "INFO - 20:23:57: EPOCH 7: training on 523538 raw words (199019 effective words) took 0.7s, 272589 effective words/s\n",
      "INFO - 20:23:57: EPOCH 8: training on 523538 raw words (199566 effective words) took 0.9s, 220568 effective words/s\n",
      "INFO - 20:23:58: EPOCH 9: training on 523538 raw words (199370 effective words) took 0.7s, 275888 effective words/s\n",
      "INFO - 20:23:59: EPOCH 10: training on 523538 raw words (199359 effective words) took 0.9s, 226393 effective words/s\n",
      "INFO - 20:24:00: EPOCH 11: training on 523538 raw words (199266 effective words) took 0.7s, 273406 effective words/s\n",
      "INFO - 20:24:01: EPOCH 12 - PROGRESS: at 100.00% examples, 198507 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 20:24:01: EPOCH 12: training on 523538 raw words (199450 effective words) took 1.0s, 198308 effective words/s\n",
      "INFO - 20:24:02: EPOCH 13 - PROGRESS: at 67.19% examples, 130615 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 20:24:02: EPOCH 13: training on 523538 raw words (199607 effective words) took 1.3s, 155338 effective words/s\n",
      "INFO - 20:24:03: EPOCH 14 - PROGRESS: at 98.23% examples, 194260 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 20:24:03: EPOCH 14: training on 523538 raw words (199154 effective words) took 1.0s, 197782 effective words/s\n",
      "INFO - 20:24:04: EPOCH 15: training on 523538 raw words (199301 effective words) took 1.0s, 207900 effective words/s\n",
      "INFO - 20:24:05: EPOCH 16: training on 523538 raw words (199383 effective words) took 0.8s, 240931 effective words/s\n",
      "INFO - 20:24:06: EPOCH 17 - PROGRESS: at 90.04% examples, 174028 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 20:24:06: EPOCH 17: training on 523538 raw words (199392 effective words) took 1.1s, 178901 effective words/s\n",
      "INFO - 20:24:07: EPOCH 18 - PROGRESS: at 96.29% examples, 192177 words/s, in_qsize 2, out_qsize 1\n",
      "INFO - 20:24:07: EPOCH 18: training on 523538 raw words (199753 effective words) took 1.0s, 198049 effective words/s\n",
      "INFO - 20:24:08: EPOCH 19: training on 523538 raw words (199573 effective words) took 0.9s, 231100 effective words/s\n",
      "INFO - 20:24:09: EPOCH 20 - PROGRESS: at 49.86% examples, 96039 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 20:24:09: EPOCH 20: training on 523538 raw words (199171 effective words) took 1.3s, 158369 effective words/s\n",
      "INFO - 20:24:10: EPOCH 21 - PROGRESS: at 82.30% examples, 162433 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 20:24:10: EPOCH 21: training on 523538 raw words (199563 effective words) took 1.2s, 173343 effective words/s\n",
      "INFO - 20:24:11: EPOCH 22 - PROGRESS: at 78.54% examples, 154044 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 20:24:12: EPOCH 22: training on 523538 raw words (199527 effective words) took 1.2s, 172521 effective words/s\n",
      "INFO - 20:24:12: EPOCH 23: training on 523538 raw words (199469 effective words) took 1.0s, 209673 effective words/s\n",
      "INFO - 20:24:13: EPOCH 24 - PROGRESS: at 86.15% examples, 171373 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 20:24:14: EPOCH 24: training on 523538 raw words (199190 effective words) took 1.1s, 175558 effective words/s\n",
      "INFO - 20:24:15: EPOCH 25: training on 523538 raw words (199584 effective words) took 0.9s, 215917 effective words/s\n",
      "INFO - 20:24:16: EPOCH 26 - PROGRESS: at 86.15% examples, 158482 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 20:24:16: EPOCH 26: training on 523538 raw words (199057 effective words) took 1.2s, 162854 effective words/s\n",
      "INFO - 20:24:17: EPOCH 27: training on 523538 raw words (199293 effective words) took 0.9s, 213147 effective words/s\n",
      "INFO - 20:24:18: EPOCH 28 - PROGRESS: at 86.15% examples, 153109 words/s, in_qsize 0, out_qsize 2\n",
      "INFO - 20:24:18: EPOCH 28: training on 523538 raw words (198837 effective words) took 1.2s, 163135 effective words/s\n",
      "INFO - 20:24:19: EPOCH 29: training on 523538 raw words (199390 effective words) took 0.8s, 238671 effective words/s\n",
      "INFO - 20:24:19: Word2Vec lifecycle event {'msg': 'training on 15706140 raw words (5980911 effective words) took 29.0s, 206272 effective words/s', 'datetime': '2023-06-08T20:24:19.296174', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-43-generic-x86_64-with-glibc2.35', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.48 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "48e12768512b82c2d5cf6a543e3a9f2515699a22"
   },
   "source": [
    "As we do not plan to train the model any further, we are calling init_sims(), which will make the model much more memory-efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "34dd51c7f2f39d016b982ef81e4df576f6b31bcb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_195837/514372312.py:1: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v_model.init_sims(replace=True)\n",
      "WARNING - 20:24:45: destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
     ]
    }
   ],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a420d5a98eb860cff1f4bbac8cbe2054459b6200"
   },
   "source": [
    "# Exploring the model\n",
    "## Most similar to:\n",
    "\n",
    "Here, we will ask our model to find the word most similar to some of the most iconic characters of the Simpsons!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a8f3cfd8ac88978a4df31c90afa194bd6fa4f3f5"
   },
   "source": [
    "![Homer_Simpson](https://vignette.wikia.nocookie.net/simpsons/images/0/02/Homer_Simpson_2006.png/revision/latest?cb=20091207194310)\n",
    "\n",
    "Let's see what we get for the show's main character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "339207a733a1ac42fe60e32a29f9e5d5ca0a9275"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('depressed', 0.7135162353515625),\n",
       " ('gee', 0.6696743965148926),\n",
       " ('creepy', 0.6566376686096191),\n",
       " ('marge', 0.6520886421203613),\n",
       " ('terrific', 0.6516083478927612),\n",
       " ('sweetheart', 0.643757700920105),\n",
       " ('hammock', 0.642941951751709),\n",
       " ('moe', 0.6417779922485352),\n",
       " ('shoulda', 0.6373036503791809),\n",
       " ('compliment', 0.6356023550033569)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"homer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b6686e6fa956a98450259b063b4cf51019a6d0b"
   },
   "source": [
    "_A small precision here:_<br>\n",
    "The dataset is the Simpsons' lines of dialogue; therefore, when we look at the most similar words from \"homer\" we do **not** necessary get his family members, personality traits, or even his most quotable words. No, we get what other characters (as Homer does not often refers to himself at the 3rd person) said along with \"homer\", such as how he feels or looks (\"depressed\"), where he is (\"hammock\"), or with whom (\"marge\").\n",
    "\n",
    "Let's see what the bigram \"homer_simpson\" gives us by comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "23e5149b19f18f4f2f456d4c72afc5c188bcfba4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('easily', 0.6567609906196594),\n",
       " ('select', 0.6506896018981934),\n",
       " ('candidate', 0.6455971002578735),\n",
       " ('poll', 0.6428175568580627),\n",
       " ('montgomery_burn', 0.6419366598129272),\n",
       " ('debate', 0.6270120143890381),\n",
       " ('committee', 0.6242663860321045),\n",
       " ('governor', 0.6197243928909302),\n",
       " ('winner', 0.6191943883895874),\n",
       " ('rainy', 0.6168389916419983)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"homer_simpson\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0e3e121e022f2f659cf97bba42cecd3f3c9afb01"
   },
   "source": [
    "![Marge](https://vignette.wikia.nocookie.net/simpsons/images/0/0b/Marge_Simpson.png/revision/latest?cb=20180626055729)\n",
    "\n",
    "What about Marge now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "22595f98c675a9697243b7e826b2840e5fc3e5f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('snuggle', 0.6661581993103027),\n",
       " ('homer', 0.6520886421203613),\n",
       " ('ned', 0.6509064435958862),\n",
       " ('patty_selma', 0.6418110728263855),\n",
       " ('decent', 0.6397137641906738),\n",
       " ('rapture', 0.6393093466758728),\n",
       " ('marry', 0.6378631591796875),\n",
       " ('badly', 0.6355270743370056),\n",
       " ('awww', 0.6318559646606445),\n",
       " ('maybe', 0.631703794002533)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"marge\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b4857ff1159695c72c22417cf52dc84e0dfc9ea"
   },
   "source": [
    "![bart](https://vignette.wikia.nocookie.net/simpsons/images/6/65/Bart_Simpson.png/revision/latest?cb=20180319061933)\n",
    "\n",
    "Let's check Bart now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "ac9ba47738e596dce6552099e76f303f28577943"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lisa', 0.7992392778396606),\n",
       " ('mom_dad', 0.691601037979126),\n",
       " ('hearing', 0.6710435152053833),\n",
       " ('mom', 0.6702009439468384),\n",
       " ('strangle', 0.668476939201355),\n",
       " ('exploit', 0.6660684943199158),\n",
       " ('convince', 0.6627126336097717),\n",
       " ('milhouse', 0.6582393646240234),\n",
       " ('pay_attention', 0.6505537033081055),\n",
       " ('substitute', 0.6499829292297363)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"bart\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9afcf59e71f797f8e4d6d4b4ce39e359b19a450"
   },
   "source": [
    "Looks like it is making sense!\n",
    "![Groundskeeper_Willie](https://vignette.wikia.nocookie.net/simpsons/images/9/9d/Groundskeeper_Willie.png/revision/latest?cb=20130424154035)\n",
    "\n",
    "Willie the groundskeeper for the last one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8b5937dfd7584f168a33060c435036cad5b390b"
   },
   "source": [
    "## Similarities:\n",
    "Here, we will see how similar are two words to each other :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "367755f5c9e00de4bc5056c978f5b50a38c1368b"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'moe_'s' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mw2v_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmoe_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtavern\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/NLP/venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1234\u001b[0m, in \u001b[0;36mKeyedVectors.similarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity\u001b[39m(\u001b[38;5;28mself\u001b[39m, w1, w2):\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between two keys.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \n\u001b[1;32m   1221\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \n\u001b[1;32m   1233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dot(matutils\u001b[38;5;241m.\u001b[39munitvec(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mw1\u001b[49m\u001b[43m]\u001b[49m), matutils\u001b[38;5;241m.\u001b[39munitvec(\u001b[38;5;28mself\u001b[39m[w2]))\n",
      "File \u001b[0;32m~/Desktop/NLP/venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_or_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/Desktop/NLP/venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/Desktop/NLP/venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'moe_'s' not present\""
     ]
    }
   ],
   "source": [
    "w2v_model.wv.similarity(\"moe_'s\", 'tavern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d31383aa2f6310a38ec671f2ca4b0fcb195551dd"
   },
   "source": [
    "Who could forget Moe's tavern? Not Barney.\n",
    "\n",
    "![](https://vignette.wikia.nocookie.net/simpsons/images/6/6c/MaggieSimpson.PNG/revision/latest?cb=20180314210204) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "349828078b5a438d93e5494478e88095913dc58e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5947409"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity('maggie', 'baby')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7d7842e5b5cab743db56ff120d75ad3974c429d8"
   },
   "source": [
    "Maggie is indeed the most renown baby in the Simpsons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "9ee5e2532214b20fef0a597bc5ad355762fcc281"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49807742"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity('bart', 'nelson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "08a999d758ac687d626b631a8ce393eaa26f41e7"
   },
   "source": [
    "Bart and Nelson, though friends, are not that close, makes sense!\n",
    "\n",
    "## Odd-One-Out:\n",
    "\n",
    "Here, we ask our model to give us the word that does not belong to the list!\n",
    "\n",
    "Between Jimbo, Milhouse, and Kearney, who is the one who is not a bully?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "d982e44d9c212b5ee09bcaebd050a725ab5e508e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 20:30:43: vectors for words {'kearney'} are not present in the model, ignoring these words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'jimbo'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match(['jimbo', 'milhouse', 'kearney'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "12d3c035c89718e70c193f095b46e38dccef6b0d"
   },
   "source": [
    "Milhouse of course!\n",
    "![](https://vignette.wikia.nocookie.net/simpsons/images/9/91/Milhouse_Van_Houten_2.png/revision/latest?cb=20180429212659)\n",
    "\n",
    "What if we compared the friendship between Nelson, Bart, and Milhouse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "cafd4a7bec6d6255ea3f5f06df951546c0d783a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nelson'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match([\"nelson\", \"bart\", \"milhouse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "01c0148dc758db74ed8078ca54e8393ada090c8c"
   },
   "source": [
    "Seems like Nelson is the odd one here!\n",
    "![](https://vignette.wikia.nocookie.net/simpsons/images/4/40/Picture0003.jpg/revision/latest?cb=20110623042517)\n",
    "\n",
    "Last but not least, how is the relationship between Homer and his two sister-in-laws?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "445912f7d89b3cb1550926be161d134e6689f54f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'homer'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match(['homer', 'patty', 'selma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df95cdff693e843ab4b4c174fea24029447573cd"
   },
   "source": [
    "Damn, they really do not like you Homer!\n",
    "\n",
    "## Analogy difference:\n",
    "Which word is to woman as homer is to marge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "812961e79dde9f2032f708755ca287c0aef838d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reason', 0.5643137693405151),\n",
       " ('certain', 0.5641007423400879),\n",
       " ('see', 0.5545346736907959)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"woman\", \"homer\"], negative=[\"marge\"], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5a4ff4a8c8c582c6d9c704a042e8cc5d18b7bd6c"
   },
   "source": [
    "\"man\" comes at the first position, that looks about right!\n",
    "\n",
    "Which word is to woman as bart is to man?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4cfef57b94b635abb58a4ff191785506c78ec9d9"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"woman\", \"bart\"], negative=[\"man\"], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ef520bd7dd974d14afcb8e69266579ac0b703714"
   },
   "source": [
    "Lisa is Bart's sister, her male counterpart!\n",
    "<img src=\"https://vignette.wikia.nocookie.net/simpsons/images/5/57/Lisa_Simpson2.png/revision/latest?cb=20180319000458\" alt=\"drawing\" width=\"100\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "773c0acc8750ba8e728ff261f2e9ec39694c245c"
   },
   "source": [
    "### t-SNE visualizations:\n",
    "t-SNE is a non-linear dimensionality reduction algorithm that attempts to represent high-dimensional data and the underlying relationships between vectors in a lower-dimensional space.<br>\n",
    "Here is a good tutorial on it: https://medium.com/@luckylwk/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.25 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from seaborn) (2.0.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from seaborn) (1.24.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from seaborn) (3.7.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.39.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/saugat/Desktop/NLP/venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "27ec46110042fc28da900b1b344ae4e0692d5dc2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22693eaa25253b38cee3c5cd5db6b6fdddb575a4"
   },
   "source": [
    "Our goal in this section is to plot our 300 dimensions vectors into 2 dimensional graphs, and see if we can spot interesting patterns.<br>\n",
    "For that we are going to use t-SNE implementation from scikit-learn.\n",
    "\n",
    "To make the visualizations more relevant, we will look at the relationships between a query word (in <font color='red'>**red**</font>), its most similar words in the model (in <font color=\"blue\">**blue**</font>), and other words from the vocabulary (in <font color='green'>**green**</font>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "489a7d160dcd92da0ce42a3b5b461368c9ffe5f1"
   },
   "outputs": [],
   "source": [
    "def tsnescatterplot(model, word, list_names):\n",
    "    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n",
    "    its list of most similar words, and a list of words.\n",
    "    \"\"\"\n",
    "    arrays = np.empty((0, 300), dtype='f')\n",
    "    word_labels = [word]\n",
    "    color_list  = ['red']\n",
    "\n",
    "    # adds the vector of the query word\n",
    "    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n",
    "    \n",
    "    # gets list of most similar words\n",
    "    close_words = model.wv.most_similar([word])\n",
    "    \n",
    "    # adds the vector for each of the closest words to the array\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('blue')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "    \n",
    "    # adds the vector for each of the words from list_names to the array\n",
    "    for wrd in list_names:\n",
    "        wrd_vector = model.wv.__getitem__([wrd])\n",
    "        word_labels.append(wrd)\n",
    "        color_list.append('green')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "        \n",
    "    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n",
    "    reduc = PCA(n_components=50).fit_transform(arrays)\n",
    "    \n",
    "    # Finds t-SNE coordinates for 2 dimensions\n",
    "    np.set_printoptions(suppress=True)\n",
    "    \n",
    "    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n",
    "    \n",
    "    # Sets everything up to plot\n",
    "    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n",
    "                       'y': [y for y in Y[:, 1]],\n",
    "                       'words': word_labels,\n",
    "                       'color': color_list})\n",
    "    \n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(9, 9)\n",
    "    \n",
    "    # Basic plot\n",
    "    p1 = sns.regplot(data=df,\n",
    "                     x=\"x\",\n",
    "                     y=\"y\",\n",
    "                     fit_reg=False,\n",
    "                     marker=\"o\",\n",
    "                     scatter_kws={'s': 40,\n",
    "                                  'facecolors': df['color']\n",
    "                                 }\n",
    "                    )\n",
    "    \n",
    "    # Adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "         p1.text(df[\"x\"][line],\n",
    "                 df['y'][line],\n",
    "                 '  ' + df[\"words\"][line].title(),\n",
    "                 horizontalalignment='left',\n",
    "                 verticalalignment='bottom', size='medium',\n",
    "                 color=df['color'][line],\n",
    "                 weight='normal'\n",
    "                ).set_size(15)\n",
    "\n",
    "    \n",
    "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "            \n",
    "    plt.title('t-SNE visualization for {}'.format(word.title()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3943c170a5f5f09974d90c89bdb9ec761e63a416"
   },
   "source": [
    "Code inspired by: [[2]](#References:)\n",
    "\n",
    "## 10 Most similar words vs. 8 Random words:\n",
    "Let's compare where the vector representation of Homer, his 10 most similar words from the model, as well as 8 random ones, lies in a 2D graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "18d788b2a92f94771a5f9485a885d44dfba62a94"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 300 and the array at index 1 has size 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtsnescatterplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw2v_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhomer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdog\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbird\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mah\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaude\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbob\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mapu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mduff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 10\u001b[0m, in \u001b[0;36mtsnescatterplot\u001b[0;34m(model, word, list_names)\u001b[0m\n\u001b[1;32m      7\u001b[0m color_list  \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# adds the vector of the query word\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# gets list of most similar words\u001b[39;00m\n\u001b[1;32m     13\u001b[0m close_words \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mmost_similar([word])\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/NLP/venv/lib/python3.10/site-packages/numpy/lib/function_base.py:5499\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5497\u001b[0m     values \u001b[38;5;241m=\u001b[39m ravel(values)\n\u001b[1;32m   5498\u001b[0m     axis \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 5499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 300 and the array at index 1 has size 100"
     ]
    }
   ],
   "source": [
    "tsnescatterplot(w2v_model, 'homer', ['dog', 'bird', 'ah', 'maude', 'bob', 'mel', 'apu', 'duff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c73fc2faaf0baecc84f02a97b50cb9ccefa48686"
   },
   "source": [
    "Interestingly, the 10 most similar words to Homer ends up around him, so does Apu and (sideshow) Bob, two recurrent characters.\n",
    "\n",
    "## 10 Most similar words vs. 10 Most dissimilar\n",
    "\n",
    "This time, let's compare where the vector representation of Maggie and her 10 most similar words from the model lies compare to the vector representation of the 10 most dissimilar words to Maggie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "10c77b072f7c281f2be919341be116565c20d8a8"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 300 and the array at index 1 has size 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtsnescatterplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw2v_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaggie\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw2v_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnegative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaggie\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 10\u001b[0m, in \u001b[0;36mtsnescatterplot\u001b[0;34m(model, word, list_names)\u001b[0m\n\u001b[1;32m      7\u001b[0m color_list  \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# adds the vector of the query word\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# gets list of most similar words\u001b[39;00m\n\u001b[1;32m     13\u001b[0m close_words \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mmost_similar([word])\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/NLP/venv/lib/python3.10/site-packages/numpy/lib/function_base.py:5499\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5497\u001b[0m     values \u001b[38;5;241m=\u001b[39m ravel(values)\n\u001b[1;32m   5498\u001b[0m     axis \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 5499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 300 and the array at index 1 has size 100"
     ]
    }
   ],
   "source": [
    "tsnescatterplot(w2v_model, 'maggie', [i[0] for i in w2v_model.wv.most_similar(negative=[\"maggie\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "87315bfbaceb3733bd7af035db6c59cfc4b1ba7f"
   },
   "source": [
    "Neat! Maggie and her most similar words form a distinctive cluster from the most dissimilar words, it is a really encouraging plot!\n",
    "\n",
    "## 10 Most similar words vs. 11th to 20th Most similar words:\n",
    "\n",
    "Finally, we are going to plot the most similar words to Mr. Burns ranked 1st to 10th versus the ones ranked 11th to 20th:\n",
    "\n",
    "(PS: Mr. Burns became mr_burn after the preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "e6f0bc598922f4f2cd17d2511560242a3c35fdd9"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 300 and the array at index 1 has size 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtsnescatterplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw2v_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmr_burn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw2v_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmr_burn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 10\u001b[0m, in \u001b[0;36mtsnescatterplot\u001b[0;34m(model, word, list_names)\u001b[0m\n\u001b[1;32m      7\u001b[0m color_list  \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# adds the vector of the query word\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# gets list of most similar words\u001b[39;00m\n\u001b[1;32m     13\u001b[0m close_words \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mmost_similar([word])\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/NLP/venv/lib/python3.10/site-packages/numpy/lib/function_base.py:5499\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5497\u001b[0m     values \u001b[38;5;241m=\u001b[39m ravel(values)\n\u001b[1;32m   5498\u001b[0m     axis \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 5499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 300 and the array at index 1 has size 100"
     ]
    }
   ],
   "source": [
    "tsnescatterplot(w2v_model, \"mr_burn\", [t[0] for t in w2v_model.wv.most_similar(positive=[\"mr_burn\"], topn=20)][10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "011aa35b717d985f1d9fb820208531222055ff7b"
   },
   "source": [
    "As we can see, and that is very nice, all the 20 words are forming one cluster around Mr. Burns.\n",
    "\n",
    "# Final Thoughts\n",
    "\n",
    "I hope you found this tutorial useful and had as much fun reading it as I had writing it. Please do not hesitate to leave any comments, questions or suggestions you might have. See you around!\n",
    "\n",
    "Also, please check [Supportiv](http://www.supportiv.com) around! (Simpson-ized logo)\n",
    "\n",
    "<img src=\"https://fontmeme.com/permalink/180904/cc3d27a8aaa88189e764ee9d02331d0d.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "\n",
    "# Materials for more in depths understanding:\n",
    "* Word Embeddings introduction: https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "* Word2Vec introduction: https://skymind.ai/wiki/word2vec\n",
    "* Another Word2Vec introduction: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "* A great Gensim implentation tutorial: http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.W467ScBjM2x\n",
    "* Original articles from Mikolov et al.: https://arxiv.org/abs/1301.3781 and https://arxiv.org/abs/1310.4546\n",
    "\n",
    "\n",
    "# Acknowledgements\n",
    "\n",
    "* [Pouria Mojabi](https://www.linkedin.com/in/pouria-mojabi-1873615/), co-fouder of Supportiv Inc.\n",
    "\n",
    "# References:\n",
    "* [1]. Neural Net picture: McCormick, C. (2016, April 19). Word2Vec Tutorial - The Skip-Gram Model. Retrieved from http://www.mccormickml.com\n",
    "* [2]. Aneesha Bakharia Medium article: https://medium.com/@aneesha/using-tsne-to-plot-a-subset-of-similar-words-from-word2vec-bb8eeaea6229\n",
    "\n",
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
